---
title: "Understanding GPT-3"
date: "2022-11-06"
tag: "Artificial Intelligence"
---

Recently, I harnessed GPT-3 whilst developing a demo of an app that provides useful drug information to a users. It was a fantastic opportunity to learn more about GPT-3 and catch a glimpse of the road our technology might be heading. But firstly, in this post I want to explore what GPT-3 actually is and how it 'knows' what it knows.

## What is GPT-3?

To parapharase many sources including OpenAI, Generative GPT-3 is an autoregressive language model that uses deep learning to make text that sounds like it was written by a person. If you give it some text as a starting point, it will come up with text that continues what you gave it.

The architecture is a standard transformer network with a context that is 2048 tokens long and has 175 billion parameters, which is a size that has never been seen before (requiring 800 GB of storage). The way it is trained is called "generative pretraining", **which means it is taught to guess what the next token will be.** On many text-based tasks, the model showed strong learning after just a few tries.

It is the third generation of the GPT-n series of language prediction models, and it came after GPT-2. It was made by OpenAI, an artificial intelligence research lab in San Francisco.

GPT-3 was released in May 2020 and was in beta testing as of July 2020. It is part of a trend in natural language processing (NLP) systems toward language representations that have already been trained.

## This can get very exciting!

-   Imagine a world in computer games that is made up of highly intelligent GPT-3's communicating with the player and themselves.
-   Imagine a highly intelligent oracle that has scowered the internet for you to provide you with accurate, detailed information. (A Google Search Engine 2.0?)

There are far more possible use cases than this, and it will be interesting to see where and how this technology will shape our society in the future.

ibi.
